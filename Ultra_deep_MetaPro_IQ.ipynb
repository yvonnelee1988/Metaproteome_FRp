{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "488a1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code is for generating Protein Content Networks (PCN), degree distributions and calculating dij values\n",
    "    # from the deep metaproteomics dataset using the protein-peptide bridge approach.\n",
    "\n",
    "### About original datafiles in the folder \\1_database_search_results:\n",
    "    ## MetaLab_peptide_n.xlsx, proteinGroups.txt, peptides.txt, function.csv are generated by database search using MetaLab\n",
    "    ## function1.txt and proteinGroups_top1.txt is a short version of the function and proteinGroups tables\n",
    "    ## by keeping the first protein of each protein group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a1f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509efd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31e97fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leyuan/opt/anaconda3/lib/python3.9/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/Users/leyuan/opt/anaconda3/lib/python3.9/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "#Step1 match peptide IDs to taxon table\n",
    "# Read MetaLab_peptide_n.xlsx tables, there were two generated by the MetaLab search, need to merge.\n",
    "taxon_table_1 = pd.read_excel('1_database_search_results/MetaLab_peptide_1.xlsx', sheet_name='peptide list') # it takes a few minutes to read the tables\n",
    "taxon_table_2 = pd.read_excel('1_database_search_results/MetaLab_peptide_2.xlsx', sheet_name='peptide list')\n",
    "taxon_table = pd.concat([taxon_table_1, taxon_table_2], axis=0)\n",
    "# Read peptides.txt that are generated by MetaLab\n",
    "peptide_table = pd.read_table('1_database_search_results/peptides.txt', sep = '\\t', low_memory=False)\n",
    "# Obtain only the peptide ids and sequences form peptide_table\n",
    "peptide_ID_match = peptide_table[['Sequence', 'id']]\n",
    "# Remove the id from taxon_table\n",
    "# Match real ids to the table\n",
    "taxon_table_drop = taxon_table.drop(columns='Peptide id')\n",
    "merge_table = taxon_table_drop.merge(peptide_ID_match, on='Sequence')\n",
    "# Generate peptide id - Sequence - taxon table\n",
    "peptIDe_to_taxon = pd.DataFrame(merge_table, columns = ['id', 'Sequence', 'LCA', 'Rank', 'Superkingdom', 'Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species'])\n",
    "peptIDe_to_taxon.to_csv('2_data_processing/1_PCN/PCN_generation/peptIDe_to_taxon_MP_deep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94395e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2 match proteinGroups to taxon table according bridged by peptide IDS\n",
    "# Need to obtain a dataframe of ProteinGroups_and_peptide_IDs\n",
    "# read proteinGroups.txt, and only keep two columns -- protein group top1 names and peptides IDs\n",
    "proteinGroups_table = pd.read_table('1_database_search_results/proteinGroups.txt', sep = '\\t', low_memory=False)\n",
    "proteinGroups_ids = proteinGroups_table[['Protein IDs', 'Peptide IDs']]\n",
    "proteinGroups_ids_split = pd.concat([proteinGroups_ids[\"Protein IDs\"].str.split(\"\\;\", n=1, expand = True).drop(columns=1).\n",
    "                                    rename(columns = {0 : 'Protein IDs'}), proteinGroups_ids[\"Peptide IDs\"].str.split(\"\\;\", expand = True)], axis = 1)\n",
    "long_matches = pd.melt(proteinGroups_ids_split, id_vars=['Protein IDs'], value_name='id').drop(columns='variable').dropna().\\\n",
    "               sort_values('Protein IDs', inplace = False).reset_index().drop(columns='index')\n",
    "long_matches['id'] = long_matches['id'].astype('int64')\n",
    "pro_pep_tax = long_matches.merge(peptIDe_to_taxon, on = 'id')\n",
    "pro_pep_tax.to_csv('2_data_processing/1_PCN/PCN_generation/pro_pep_tax_MP_deep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac3f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3 match proteinGroups to function\n",
    "#read unique genus matches table.\n",
    "#Used Excel to obtain the unique matches (for details, see Supplementary Note 1), and saved the table as 'pro_pep_tax_MP_deep_unique.csv'\n",
    "protein_taxon_table = pd.read_table('2_data_processing/1_PCN/PCN_generation/pro_pep_tax_MP_deep_unique.csv', sep = ',')\n",
    "#read function table, and combine with protein group table, then unique genus matches table\n",
    "## The function table contains a column that is a combination of KEGG and COG, \n",
    "## first proteins were matched to KEGG using GhostKOALA. The remaining proteins \n",
    "## without a KEGG ko match was complemented with a COG from the functional output of MetaLab.\n",
    "function_table = pd.read_table('1_database_search_results/function_COG_KEGG.txt', sep = '\\t', low_memory=False).rename(columns = {'Name': 'Protein IDs'})\n",
    "protein_top1_table = pd.read_table('1_database_search_results/proteinGroups1.txt', sep = '\\t', low_memory=False)\n",
    "function_table_top1 = function_table.merge(protein_top1_table, on = 'Protein IDs')\n",
    "merge_pro_tax_fun = protein_taxon_table.merge(function_table_top1, on = 'Protein IDs')\n",
    "merge_pro_tax_fun.to_csv('2_data_processing/1_PCN/PCN_generation/Full_table_pro_tax_fun_MP_deep_KEGG_COG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cad1130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HM454\n",
      " HM455\n",
      " HM466\n",
      " HM503\n"
     ]
    }
   ],
   "source": [
    "#Step4 generate PCNs based on Functions in the funtion table\n",
    "pro_gen_cog = pd.DataFrame(merge_pro_tax_fun, columns = ['Protein IDs', 'Genus', 'KEGG_COG'])\n",
    "proteinGroups_table = pd.read_table('1_database_search_results/proteinGroups.txt', sep = '\\t', low_memory=False)\n",
    "proteinGroups_intensity = pd.concat([proteinGroups_table[\"Protein IDs\"].str.split(\"\\;\", n=1, expand = True).drop(columns=1).rename(columns = {0 : 'Protein IDs'}),\n",
    "                                     proteinGroups_table.filter(regex='Intensity HM')], axis = 1)\n",
    "pro_gen_cog_int = pro_gen_cog.merge(proteinGroups_intensity, on = 'Protein IDs')\n",
    "pro_gen_cog_int.columns=pro_gen_cog_int.columns.str.replace('Intensity','')\n",
    "pro_gen_cog_int.to_csv('2_data_processing/1_PCN/PCN_generation/Full_table_pro_tax_fun_KEGG_COG_for_PCN.csv')\n",
    "#Use loop to generate and save PCN for each sample\n",
    "for x in pro_gen_cog_int.columns[3:7]:\n",
    "    # get columns for this specific column\n",
    "    pro_gen_cog_sam = pd.DataFrame(pro_gen_cog_int, columns = ['Genus', 'KEGG_COG', x])\n",
    "    # print(pro_gen_cog_sam.columns)\n",
    "    # do sum by groups\n",
    "    pro_gen_cog_sam_sum = pro_gen_cog_sam.groupby(['Genus', 'KEGG_COG']).sum().reset_index()\n",
    "    # reshape to wide table\n",
    "    pro_gen_cog_sam_sum_wide = pro_gen_cog_sam_sum.pivot(index='KEGG_COG', columns='Genus', values= x )\n",
    "    # write to tables\n",
    "    pro_gen_cog_sam_sum_wide.to_csv('2_data_processing/1_PCN/PCN_tables_raw/PCN_KEGG_COG' + x + '.csv')\n",
    "    print(x)\n",
    "#endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d83d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_raw/PCN_KEGG_COG HM503.csv\n",
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_raw/PCN_KEGG_COG HM466.csv\n",
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_raw/PCN_KEGG_COG HM455.csv\n",
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_raw/PCN_KEGG_COG HM454.csv\n"
     ]
    }
   ],
   "source": [
    "#Step 5. Generate 0 1 PCN tables\n",
    "### Batch process files - from genus to COG tables, generate an overall PCN and individual sample PCNs ########\n",
    "def findfile(path, tagstr):\n",
    "    #setup an empty dataframe with same index and column names as all tables\n",
    "    path = os.path.abspath(path)\n",
    "    for x in os.listdir(path):\n",
    "        fulldir = os.path.join(path, x)  # join to absolute path\n",
    "        if os.path.isfile(fulldir):  # file, match -> print\n",
    "            # if os.path.split(fulldir)[1]==tagstr   # look for the designated file\n",
    "            if tagstr in os.path.splitext(fulldir)[1]:  # look for files containing the key word\n",
    "                print(os.path.join(os.path.abspath(fulldir), fulldir))\n",
    "                PCN_table_single = pd.read_table(fulldir, delimiter=\",\").set_index(['KEGG_COG']).fillna(0).T\n",
    "                if_detected = PCN_table_single.astype('float') > 0\n",
    "                PCN_table_single['num_detected'] = if_detected.sum(axis=1)\n",
    "                PCN_table_single_filtered = PCN_table_single.loc[(PCN_table_single.sum(axis=1) != 0), (PCN_table_single.sum(axis=0) != 0)].\\\n",
    "                    sort_values(by ='num_detected', ascending=False).drop(columns='num_detected') #remove empty columns for individual files\n",
    "                PCN_table_single_filtered = PCN_table_single_filtered.astype(\n",
    "                    'float') > 0  # replace values with presence or not (1 or 0), it will give a True or False value\n",
    "                PCN_table_single_filtered = PCN_table_single_filtered.astype('float')  # replace True or False with 1 or 0\n",
    "                PCN_table_single_filtered_sum = pd.DataFrame(PCN_table_single_filtered.sum(axis=0)).rename(columns={0: 'sum_num'}).T\n",
    "                PCN_table_single_filtered_sum_col = pd.concat([PCN_table_single_filtered, PCN_table_single_filtered_sum]).sort_values(by='sum_num', axis=1,\n",
    "                                                                                            ascending=False).drop(\n",
    "                    index='sum_num') # get sorted PCN for each individual sample\n",
    "                PCN_table_single_filtered_sum_col.to_csv('2_data_processing/1_PCN/PCN_tables_0_1/' + os.path.splitext(x)[0] + '_PCN_0_1' + '.csv')\n",
    "\n",
    "findfile('2_data_processing/1_PCN/PCN_tables_raw/', '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6de8d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_0_1/PCN_KEGG_COG HM503_PCN_0_1.csv\n",
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_0_1/PCN_KEGG_COG HM455_PCN_0_1.csv\n",
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_0_1/PCN_KEGG_COG HM466_PCN_0_1.csv\n",
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_0_1/PCN_KEGG_COG HM454_PCN_0_1.csv\n"
     ]
    }
   ],
   "source": [
    "## Part 2. Degree distributions\n",
    "# Degree distributions\n",
    "def findfile(path, tagstr):\n",
    "    path = os.path.abspath(path)\n",
    "\n",
    "    for x in os.listdir(path):\n",
    "\n",
    "        fulldir = os.path.join(path, x)  # join to absolute path\n",
    "\n",
    "        if os.path.isfile(fulldir):  # file, match -> print\n",
    "            # if os.path.split(fulldir)[1]==tagstr   # look for the designated file\n",
    "            if tagstr in os.path.splitext(fulldir)[1]:  # look for files containing the key word\n",
    "                print(os.path.join(os.path.abspath(fulldir), fulldir))\n",
    "\n",
    "                ######## Calculation starts here ###############################\n",
    "                origin_data = pd.read_table(fulldir, delimiter=\",\", index_col=0)           #read a PCN_0_1 table\n",
    "\n",
    "                taxon_degree_distribution = pd.DataFrame()\n",
    "                COG_degree_distribution = pd.DataFrame()\n",
    "\n",
    "                # calculate number of nodes k degree\n",
    "                taxon_degree_distribution['k'] = origin_data.sum(axis=1)\n",
    "                COG_degree_distribution['k'] = origin_data.sum(axis=0)\n",
    "\n",
    "                # calculate pk fraction of nodes\n",
    "                taxon_pk = pd.DataFrame(columns=['k'])\n",
    "                taxon_pk['pk'] = taxon_degree_distribution['k'].value_counts() / len(taxon_degree_distribution.index)\n",
    "                taxon_pk['k'] = taxon_pk.index\n",
    "                taxon_degree_distribution = taxon_pk.merge(taxon_degree_distribution, on='k')\n",
    "\n",
    "                COG_pk = pd.DataFrame(columns=['k'])\n",
    "                COG_pk['pk'] = COG_degree_distribution['k'].value_counts() / len(COG_degree_distribution.index)\n",
    "                COG_pk['k'] = COG_pk.index\n",
    "                COG_degree_distribution = COG_pk.merge(COG_degree_distribution, on='k')\n",
    "\n",
    "                taxon_degree_distribution.to_csv('2_data_processing/2_Degree_distribution/Taxon/'+ x)\n",
    "                COG_degree_distribution.to_csv('2_data_processing/2_Degree_distribution/Function/'+ x)\n",
    "\n",
    "\n",
    "findfile('2_data_processing/1_PCN/PCN_tables_0_1', '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3db09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_raw/PCN_KEGG_COG HM503.csv\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "/Users/leyuan/Documents/lab/paper/FR/NatMethods/V2/codes/FRp_codes_revised/2_data_processing/1_PCN/PCN_tables_raw/PCN_KEGG_COG HM466.csv\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "### Part 3. deep MP dataset, all genera, functional distance ###\n",
    "# the results will be used for FR calculation, but not for dij visualization\n",
    "def findfile(path, tagstr):\n",
    "    path = os.path.abspath(path)\n",
    "\n",
    "    for x in os.listdir(path):\n",
    "\n",
    "        fulldir = os.path.join(path, x)  # join to absolute path\n",
    "\n",
    "        if os.path.isfile(fulldir):  # file, match -> print\n",
    "            # if os.path.split(fulldir)[1]==tagstr   # look for the designated file\n",
    "            if tagstr in os.path.splitext(fulldir)[1]:  # look for files containing the key word\n",
    "                print(os.path.join(os.path.abspath(fulldir), fulldir))\n",
    "\n",
    "                ######## Calculation starts here ###############################\n",
    "                origin_data = pd.read_table(fulldir, delimiter=\",\", index_col=0).fillna(0) #read a PCN table and fill nan with zero\n",
    "\n",
    "                # Protein content network\n",
    "                PCN = origin_data # PCN without any normalization\n",
    "                PCN = PCN[PCN.columns[PCN.sum()>0]] # remove genus column if sum to zero\n",
    "                PCN_taxa_norm = PCN.apply(lambda x: x/sum(x), axis = 0).fillna(0) # Proteomic content network / normalized to each taxon\n",
    "\n",
    "                PCN_taxa_norm_T = PCN_taxa_norm.T\n",
    "                if_detected = PCN_taxa_norm_T > 0  # turn PCN numbers to 0 or 1\n",
    "                PCN_taxa_norm_T['num_detected'] = if_detected.sum(axis=1)  # obtain num_detected,\n",
    "                PCN_taxa_norm_all = PCN_taxa_norm_T.fillna(0).drop(columns='num_detected').T\n",
    "\n",
    "                # create an empty dataframe\n",
    "                dij_matrix_norm = pd.DataFrame()\n",
    "                dij_list_norm = pd.DataFrame(\n",
    "                    index=range((len(PCN_taxa_norm_all.columns)) * (len(PCN_taxa_norm_all.columns) - 1) // 2),\n",
    "                    columns=['dij_list'])\n",
    "                dij_list_name_norm = pd.DataFrame(\n",
    "                    index=range((len(PCN_taxa_norm_all.columns)) * (len(PCN_taxa_norm_all.columns) - 1) // 2),\n",
    "                    columns=['VS'])\n",
    "                dij_num_norm = 0\n",
    "                # getting a dij between any two bacteria genera\n",
    "                for i in range(0, len(PCN_taxa_norm_all.columns)):\n",
    "                    Gi = PCN_taxa_norm_all.iloc[:, i]\n",
    "                    print(i)\n",
    "                    for j in range(0, len(PCN_taxa_norm_all.columns)):\n",
    "                        Gj = PCN_taxa_norm_all.iloc[:, j]\n",
    "                        Gij = pd.concat([Gi, Gj], axis=1)\n",
    "                        dij = 1 - sum(Gij.apply(lambda x: min(x), axis=1)) / sum(Gij.apply(lambda x: max(x), axis=1))\n",
    "                        if i > j:\n",
    "                            dij_matrix_norm.at[Gij.columns.values[0], Gij.columns.values[1]] = dij\n",
    "                            dij_list_norm.at[dij_num_norm, 'dij_list'] = dij\n",
    "                            dij_num_norm = dij_num_norm + 1\n",
    "                            dij_list_name_norm.at[dij_num_norm-1] = Gij.columns.values[0] + '_vs_' + Gij.columns.values[1]\n",
    "                dij_list_summary_norm = pd.concat([dij_list_name_norm, dij_list_norm], axis=1)\n",
    "                dij_matrix_norm.to_csv('2_data_processing/3_PCN_Functional_distance/all/norm/matrix/all_norm_dij_matrix_'+ x)\n",
    "                dij_list_summary_norm.to_csv('2_data_processing/3_PCN_Functional_distance/all/norm/list/all_norm_dij_list_'+ x)\n",
    "                # endregion\n",
    "findfile('2_data_processing/1_PCN/PCN_tables_raw', '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0cc31db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leyuan/Documents/lab/data_analysis/20210310_FR_study/NC_revise_20221228/Ultra_deep_MetaPro_IQ/2_data_processing/3_PCN_Functional_distance/all/norm/list/all_norm_dij_list_PCN_KEGG_COG HM503.csv\n",
      "0.03445541738848285\n",
      "0.39772851423029276\n",
      "/Users/leyuan/Documents/lab/data_analysis/20210310_FR_study/NC_revise_20221228/Ultra_deep_MetaPro_IQ/2_data_processing/3_PCN_Functional_distance/all/norm/list/all_norm_dij_list_PCN_KEGG_COG HM466.csv\n",
      "0.10212832444431968\n",
      "0.6682941421468552\n",
      "/Users/leyuan/Documents/lab/data_analysis/20210310_FR_study/NC_revise_20221228/Ultra_deep_MetaPro_IQ/2_data_processing/3_PCN_Functional_distance/all/norm/list/all_norm_dij_list_PCN_KEGG_COG HM454.csv\n",
      "0.12424754019234395\n",
      "0.7478833576642722\n",
      "/Users/leyuan/Documents/lab/data_analysis/20210310_FR_study/NC_revise_20221228/Ultra_deep_MetaPro_IQ/2_data_processing/3_PCN_Functional_distance/all/norm/list/all_norm_dij_list_PCN_KEGG_COG HM455.csv\n",
      "0.12456017177383634\n",
      "0.7169178609450153\n"
     ]
    }
   ],
   "source": [
    "# Calculates functional redundancy following this equation:\n",
    "# FR = ∑(N,i=1)∑(N,j=1)(1-dij)pipj\n",
    "\n",
    "## loop for all samples - deep metaproteomics\n",
    "######## Batch process files - calculating Dij for each sample ########\n",
    "\n",
    "genus_table_original = pd.read_table(\"1_database_search_results/Taxonomy_table_genus_level_metaproteomics.csv\",\n",
    "                                     delimiter=\",\", index_col=0)\n",
    "genus_table_p_matrix = genus_table_original.div(genus_table_original.sum(axis=0), axis=1)\n",
    "\n",
    "## loop for all samples - deep metaproteomics\n",
    "######## Batch process files - calculating Dij for each sample ########\n",
    "def findfile(path, tagstr):\n",
    "    Record_FR = pd.DataFrame()\n",
    "    Record_FR_norm = pd.DataFrame()\n",
    "    count = 0\n",
    "\n",
    "    path = os.path.abspath(path)\n",
    "\n",
    "    for x in os.listdir(path):\n",
    "\n",
    "        fulldir = os.path.join(path, x)  # join to absolute path\n",
    "\n",
    "        if os.path.isfile(fulldir):  # file, match -> print\n",
    "            # if os.path.split(fulldir)[1]==tagstr   # look for the designated file\n",
    "            if tagstr in os.path.splitext(fulldir)[1]:  # look for files containing the key word\n",
    "                print(os.path.join(os.path.abspath(fulldir), fulldir))\n",
    "\n",
    "                ######## Calculation starts here ###############################\n",
    "                dij_table_data = pd.read_table(fulldir, delimiter=\",\", index_col=0)\n",
    "                dij_table_data_indexed = pd.concat(\n",
    "                    [dij_table_data, dij_table_data[\"VS\"].str.split(\"_vs_\", n=1, expand=True)], axis=1).rename(\n",
    "                    columns={0: \"i\", 1: \"j\"})\n",
    "                # index to the sample column in the genus_table_p_matrix\n",
    "                genus_table_p_sample_i = genus_table_p_matrix[[x[-9:-4]]].rename(columns={x[-9:-4]: 'Abundance'})\n",
    "                genus_table_p_sample_j = genus_table_p_matrix[[x[-9:-4]]].rename(columns={x[-9:-4]: 'Abundance'})\n",
    "\n",
    "                # assign pi and pj values to the data table\n",
    "                genus_table_p_sample_i = genus_table_p_sample_i.reset_index().rename(columns={'Name': \"i\"})\n",
    "                genus_table_p_sample_j = genus_table_p_sample_j.reset_index().rename(columns={'Name': \"j\"})\n",
    "                dij_table_trial_indexed_p = dij_table_data_indexed.merge(genus_table_p_sample_i, on='i').rename(\n",
    "                    columns={'Abundance': 'pi'})\n",
    "                dij_table_trial_indexed_p = dij_table_trial_indexed_p.merge(genus_table_p_sample_j, on='j').rename(\n",
    "                    columns={'Abundance': 'pj'})\n",
    "\n",
    "                # Calculate (1-dij)pipj\n",
    "                FR = 0\n",
    "                GSI = 0\n",
    "                for x in dij_table_trial_indexed_p.index:\n",
    "                    FR_ij = (1 - dij_table_trial_indexed_p.loc[x, 'dij_list']) * dij_table_trial_indexed_p.loc[\n",
    "                        x, 'pi'] * dij_table_trial_indexed_p.loc[x, 'pj']\n",
    "                    FR = FR + FR_ij * 2\n",
    "                    # Taxonomic diversity: Gini-Simpson index\n",
    "                    GSI_ij = dij_table_trial_indexed_p.loc[x, 'pi'] * dij_table_trial_indexed_p.loc[x, 'pj']\n",
    "                    GSI = GSI + GSI_ij * 2\n",
    "                print(FR)\n",
    "                print(GSI)\n",
    "                Record_FR.loc[count, 0] = fulldir[-9:-4]\n",
    "                Record_FR.loc[count, 1] = FR\n",
    "                Record_FR.loc[count, 2] = FR/GSI # Functional redundancy normalized by taxonomic diveristy\n",
    "                Record_FR.loc[count, 3] = GSI\n",
    "                Record_FR.loc[count, 4] = GSI-FR\n",
    "                count = count + 1\n",
    "\n",
    "    Record_FR.rename(columns={0: \"Sample\", 1: \"FR\", 2: \"nFR\", 3: \"TD\", 4: \"FD\"}).to_csv('2_data_processing/4_Functional_redundancy/' + 'Record_FR_PCN.csv')\n",
    "\n",
    "\n",
    "findfile('2_data_processing/3_PCN_Functional_distance/all/norm/list', '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796354e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
